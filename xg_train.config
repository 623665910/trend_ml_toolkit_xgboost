[xg_conf]
# DO NOT DELET OR ADD ANY PARAMETERS HERE. IF YOU HAVE TO, PLEASE REVISE THE CODE: xg_train.py

# ==========   General Parameters, see comment for each definition  ===========
# choose the booster, can be gbtree or gblinear
booster = gbtree
# Do not show the detailed information[1 Yes, 0 NO]
silent = 1
#  MultiThread
nthread = 10

# ==========   Tree Booster Parameters   ====================
# step size shrinkage
eta = 1.0
# minimum loss reduction required to make a further partition
gamma = 0.2
# maximum depth of a tree
max_depth = 10
# minimum sum of instance weight(hessian) needed in a child
min_child_weight = 1
max_delta_step = 0
subsample = 1
colsample_bytree = 1
colsample_bylevel = 1
lambda = 300
alpha = 0
sketch_eps = 0.03
scale_pos_weight = 900
refresh_leaf = 1



# ===============   Task Parameters   =================
# choose logistic regression loss function for binary classification
objective = binary:logistic
base_score = 0.5
eval_metric = map
eval_metric_sk = average_precision
seed = 0
# =============== common Parameters ====================
# the number of round to do boosting
num_round = 10
# 0 means do not save any model except the final round model
save_period = 0
# The path of training data
# Is the training data xg format? [1 Yes, 0 No]
xgmat = 1
data = Data/NN_train.txt
label = Data/NNAI_train.txt
xgdata = Data/NN_train.txt.xgmat
# eval: show the train error in each round[0 no]
eval = 1
cv = 5
n_jobs = 1

#===============  parameters need to be tuned =================
t_num_round = 7
t_max_depth = 7,11
t_subsample = 0.85,0.95
t_min_child_weight = 0.85,0.95
t_colsample_bytree = 0.85,0.95


